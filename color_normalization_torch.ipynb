{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "color_normalization_torch.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "TDh_quXfBuYi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import math\n",
        "import copy\n",
        "\n",
        "import six\n",
        "import torch\n",
        "import cv2\n",
        "import numpy as np\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "# from utils import toTensor, tensor_to_np\n",
        "\n",
        "device = torch.device(\"cuda:0\")\n",
        "\n",
        "# content_weight = 5\n",
        "# style_weight = 100\n",
        "# num_iterations = 1000\n",
        "# normalize_gradients = False\n",
        "\n",
        "content_layers_default = ['conv_3']\n",
        "style_layers_default = ['conv_1', 'conv_2', 'conv_3', 'conv_4', 'conv_5']\n",
        "\n",
        "\n",
        "class Normalization(nn.Module):\n",
        "    def __init__(self, mean, std):\n",
        "        super(Normalization, self).__init__()\n",
        "        self.mean = torch.tensor(mean).view(-1, 1, 1)\n",
        "        self.std = torch.tensor(std).view(-1, 1, 1)\n",
        "\n",
        "    def forward(self, img):\n",
        "        # normalize img\n",
        "        return (img - self.mean) / self.std\n",
        "\n",
        "\n",
        "def gram_matrix(input):\n",
        "    a, b, c, d = input.size()\n",
        "\n",
        "    features = input.view(a * b, c * d)\n",
        "\n",
        "    G = torch.mm(features, features.t())\n",
        "\n",
        "    # we 'normalize' the values of the gram matrix\n",
        "    # by dividing by the number of element in each feature maps.\n",
        "    return G.div(a * b * c * d)\n",
        "\n",
        "\n",
        "class ContentLoss(nn.Module):\n",
        "\n",
        "    def __init__(self, target, mask, weight):\n",
        "        super(ContentLoss, self).__init__()\n",
        "        self.target = target.detach()\n",
        "        self.mask = mask.clone()\n",
        "        self.weight = weight\n",
        "        self.loss = 0\n",
        "\n",
        "    def forward(self, input):\n",
        "        self.loss = F.mse_loss(input, self.target) * self.weight\n",
        "        return input\n",
        "\n",
        "    def content_hook(self, module, grad_input, grad_output):\n",
        "        self.mask = self.mask[:, 0:1, :, :]\n",
        "\n",
        "        # print('Inside ' + module.__class__.__name__ + ' backward')\n",
        "        #\n",
        "        # print('grad_input size:', grad_input[0].size())\n",
        "        # print('grad_output size:', grad_output[0].size())\n",
        "        # assert grad_input[0].shape == self.mask.shape, \\\n",
        "        #     'grad_input:{} is not matchable with mask:{}'.format(grad_input[0].shape, self.mask.shape)\n",
        "\n",
        "        # grad_input_1 = grad_input[0].div(torch.norm(grad_input[0], 1) + 1e-8)\n",
        "        # grad_input_1 = grad_input_1 * self.weight\n",
        "        # grad_input_1 = grad_input_1 * self.mask\n",
        "        # grad_input = tuple([grad_input_1])\n",
        "\n",
        "        grad_input_1 = grad_input[0].div(torch.norm(grad_input[0], 1) + 1e-8)\n",
        "        grad_input_1 = grad_input_1 * self.weight\n",
        "        grad_input_1 = grad_input_1 * self.mask\n",
        "        grad_input = tuple([grad_input_1, grad_input[1], grad_input[2]])\n",
        "        return grad_input\n",
        "\n",
        "\n",
        "class StyleLoss(nn.Module):\n",
        "\n",
        "    def __init__(self, target, mask, weight):\n",
        "        super(StyleLoss, self).__init__()\n",
        "        self.target = gram_matrix(target).detach()\n",
        "        self.mask = mask.clone()\n",
        "        self.weight = weight\n",
        "        self.loss = 0\n",
        "\n",
        "    def forward(self, input):\n",
        "        self.mask = self.mask[:, 0:1, :, :]\n",
        "        self.mask = self.mask.expand_as(input)\n",
        "\n",
        "        # assert section\n",
        "        # assert input.size()[:] == self.mask.size()[:], \\\n",
        "        #     'the input-size:{} is not matchable with mask-size:{}'.format(input.size()[2:], self.mask.size()[2:])\n",
        "\n",
        "        G = gram_matrix(input * self.mask)\n",
        "        G.div(self.mask.sum())\n",
        "        self.target = self.target.div(self.mask.sum())\n",
        "        self.loss = F.mse_loss(G, self.target) * self.weight\n",
        "\n",
        "        # G = gram_matrix(input)\n",
        "        # self.loss = F.mse_loss(G, self.target) * self.weight\n",
        "\n",
        "        return input\n",
        "\n",
        "    def style_hook(self, module, grad_input, grad_output):\n",
        "        self.mask = self.mask[:, 0:1, :, :]\n",
        "\n",
        "        # print('Inside ' + module.__class__.__name__ + ' backward')\n",
        "        #\n",
        "        # print('grad_input size:', grad_input[0].size())\n",
        "        # print('grad_output size:', grad_output[0].size())\n",
        "\n",
        "        assert grad_input[0].shape == self.mask.shape, \\\n",
        "            'grad_input:{} is not matchable with mask:{}'.format(grad_input[0].shape, self.mask.shape)\n",
        "\n",
        "        grad_input_1 = grad_input[0].div(torch.norm(grad_input[0], 1) + 1e-8)\n",
        "        grad_input_1 = grad_input_1 * self.weight\n",
        "        grad_input_1 = grad_input_1 * self.mask\n",
        "        grad_input = tuple([grad_input_1, grad_input[1], grad_input[2]])\n",
        "\n",
        "        # grad_input_1 = grad_input[0].div(torch.norm(grad_input[0], 1) + 1e-8)\n",
        "        # grad_input_1 = grad_input_1 * self.weight\n",
        "        # grad_input_1 = grad_input_1 * self.mask\n",
        "        # grad_input = tuple([grad_input_1])\n",
        "        return grad_input\n",
        "\n",
        "\n",
        "class TVLoss(nn.Module):\n",
        "    def __init__(self, strength):\n",
        "        super(TVLoss, self).__init__()\n",
        "        self.strength = strength\n",
        "        self.x_diff = torch.Tensor()\n",
        "        self.y_diff = torch.Tensor()\n",
        "\n",
        "    def forward(self, input):\n",
        "        self.x_diff = input[:, :, 1:, :] - input[:, :, :-1, :]\n",
        "        self.y_diff = input[:, :, :, 1:] - input[:, :, :, :-1]\n",
        "        self.loss = self.strength * (torch.sum(torch.abs(self.x_diff)) + torch.sum(torch.abs(self.y_diff)))\n",
        "        return input\n",
        "\n",
        "\n",
        "def get_model_and_losses(cnn, normalization_mean, normalization_std,\n",
        "                         style_img, content_img, mask_image, tmask_image,\n",
        "                         style_weight=100, content_weight=5, tv_weight=1e-3,\n",
        "                         content_layers=content_layers_default,\n",
        "                         style_layers=style_layers_default):\n",
        "    content_losses = []\n",
        "    style_losses = []\n",
        "\n",
        "    normalization = Normalization(normalization_mean, normalization_std).to(device)\n",
        "    model = nn.Sequential(normalization)\n",
        "    tv_loss = None\n",
        "\n",
        "    if tv_weight > 0:\n",
        "        tv_loss = TVLoss(tv_weight)\n",
        "        model.add_module('tv_loss', tv_loss)\n",
        "    # model.add_module('tv_loss', TVLoss(tv_weight))\n",
        "\n",
        "    i = 0\n",
        "    for layer in cnn.children():\n",
        "\n",
        "        if isinstance(layer, nn.Conv2d):\n",
        "            sap = nn.AvgPool2d(kernel_size=3, stride=1, padding=1)\n",
        "            if not isinstance(mask_image, torch.Tensor):\n",
        "                mask_image = toTensor(mask_image).to(device)\n",
        "            mask_image = sap(mask_image)\n",
        "            i += 1\n",
        "            name = \"conv_\" + str(i)\n",
        "            model.add_module(name, layer)\n",
        "\n",
        "        # why every time we resize the mask image to a smaller image,\n",
        "        # because later we need mask image to fit input image in deep layers\n",
        "        # vgg19 only shrink image size in pooling layer and the rate is 1/2!\n",
        "        elif isinstance(layer, nn.MaxPool2d):\n",
        "            if isinstance(mask_image, torch.Tensor):\n",
        "                mask_image = tensor_to_np(mask_image)\n",
        "            mask_image = cv2.resize(mask_image,\n",
        "                                    (math.floor(mask_image.shape[1] / 2), math.floor(mask_image.shape[0] / 2)))\n",
        "            mask_image = toTensor(mask_image).to(device)\n",
        "            name = \"pool_\" + str(i)\n",
        "            model.add_module(name, layer)\n",
        "\n",
        "        elif isinstance(layer, nn.ReLU):\n",
        "            name = \"relu_\" + str(i)\n",
        "            model.add_module(name, nn.ReLU(inplace=False))\n",
        "\n",
        "        if name in content_layers:\n",
        "            print('-----Setting up content layer-----')\n",
        "            target = model(content_img).detach()\n",
        "            content_loss = ContentLoss(target, mask_image, content_weight)\n",
        "            content_loss.register_backward_hook(content_loss.content_hook)\n",
        "            model.add_module(\"content_loss_\" + str(i), content_loss)\n",
        "            content_losses.append(content_loss)\n",
        "\n",
        "        if name in style_layers:\n",
        "            print('-----Setting up style layer-----')\n",
        "            # content_target = model(content_img).detach()\n",
        "\n",
        "            target_feature = model(style_img).detach()\n",
        "            mask = mask_image[:, 0:1, :, :]\n",
        "            mask = mask.expand_as(target_feature)\n",
        "            target_feature = target_feature * mask\n",
        "\n",
        "            # add a histogram match here\n",
        "            style_loss = StyleLoss(target_feature, mask_image, style_weight)\n",
        "            style_loss.register_backward_hook(style_loss.style_hook)\n",
        "            model.add_module(\"style_loss\" + str(i), style_loss)\n",
        "            style_losses.append(style_loss)\n",
        "\n",
        "    for i in range(len(model) - 1, -1, -1):\n",
        "        if isinstance(model[i], ContentLoss) or isinstance(model[i], StyleLoss):\n",
        "            break\n",
        "\n",
        "        model = model[:i]\n",
        "\n",
        "    return model, style_losses, content_losses, tv_loss\n",
        "\n",
        "\n",
        "def original_color(content, generated):\n",
        "    generated_y = cv2.cvtColor(generated, cv2.COLOR_BGR2YUV)[:, :, 0]\n",
        "    content_uv = cv2.cvtColor(content, cv2.COLOR_BGR2YUV)[:, :, 1:2]\n",
        "    combined_image = cv2.cvtColor(np.stack((generated_y, content_uv), 1), cv2.COLOR_YUV2BGR)\n",
        "    return combined_image\n",
        "\n",
        "\n",
        "def histogram_match(input, target, patch, stride):\n",
        "    n1, c1, h1, w1 = input.size()\n",
        "    n2, c2, h2, w2 = target.size()\n",
        "    input.resize_(h1 * w1 * h2 * w2)\n",
        "    target.resize_(h2 * w2 * h2 * w2)\n",
        "    conv = torch.tensor((), dtype=torch.float32)\n",
        "    conv = conv.new_zeros((h1 * w1, h2 * w2))\n",
        "    conv.resize_(h1 * w1 * h2 * w2)\n",
        "    assert c1 == c2, 'input:c{} is not equal to target:c{}'.format(c1, c2)\n",
        "\n",
        "    size1 = h1 * w1\n",
        "    size2 = h2 * w2\n",
        "    N = h1 * w1 * h2 * w2\n",
        "    print('N is', N)\n",
        "\n",
        "    for i in range(0, N):\n",
        "        i1 = i / size2\n",
        "        i2 = i % size2\n",
        "        x1 = i1 % w1\n",
        "        y1 = i1 / w1\n",
        "        x2 = i2 % w2\n",
        "        y2 = i2 / w2\n",
        "        kernal_radius = int((patch - 1) / 2)\n",
        "\n",
        "        conv_result = 0\n",
        "        norm1 = 0\n",
        "        norm2 = 0\n",
        "        dy = -kernal_radius\n",
        "        dx = -kernal_radius\n",
        "        while dy <= kernal_radius:\n",
        "            while dx <= kernal_radius:\n",
        "                xx1 = x1 + dx\n",
        "                yy1 = y1 + dy\n",
        "                xx2 = x2 + dx\n",
        "                yy2 = y2 + dy\n",
        "                if 0 <= xx1 < w1 and 0 <= yy1 < h1 and 0 <= xx2 < w2 and 0 <= yy2 < h2:\n",
        "                    _i1 = yy1 * w1 + xx1\n",
        "                    _i2 = yy2 * w2 + xx2\n",
        "                    for c in range(0, c1):\n",
        "                        term1 = input[int(c * size1 + _i1)]\n",
        "                        term2 = target[int(c * size2 + _i2)]\n",
        "                        conv_result += term1 * term2\n",
        "                        norm1 += term1 * term1\n",
        "                        norm2 += term2 * term2\n",
        "                dx += stride\n",
        "            dy += stride\n",
        "        norm1 = math.sqrt(norm1)\n",
        "        norm2 = math.sqrt(norm2)\n",
        "        conv[i] = conv_result / (norm1 * norm2 + 1e-9)\n",
        "\n",
        "    match = torch.tensor((), dtype=torch.float32)\n",
        "    match = match.new_zeros(input.size())\n",
        "\n",
        "    correspondence = torch.tensor((), dtype=torch.int16)\n",
        "    correspondence.new_zeros((h1, w1, 2))\n",
        "    correspondence.resize_(h1 * w1 * 2)\n",
        "\n",
        "    for id1 in range(0, size1):\n",
        "        conv_max = -1e20\n",
        "        for y2 in range(0, h2):\n",
        "            for x2 in range(0, w2):\n",
        "                id2 = y2 * w2 + x2\n",
        "                id = id1 * size2 + id2\n",
        "                conv_result = conv[id1]\n",
        "\n",
        "                if conv_result > conv_max:\n",
        "                    conv_max = conv_result\n",
        "                    correspondence[id1 * 2 + 0] = x2\n",
        "                    correspondence[id1 * 2 + 1] = y2\n",
        "\n",
        "                    for c in range(0, c1):\n",
        "                        match[c * size1 + id1] = target[c * size2 + id2]\n",
        "\n",
        "    match.resize_((n1, c1, h1, w1))\n",
        "\n",
        "    return match, correspondence\n",
        "\n",
        "\n",
        "def patch_match(x, y, mask, patch_size=3, radius=3, stride=1):\n",
        "    batch, channels, height, width = x.size()\n",
        "\n",
        "    y_pad = F.pad(y, (radius // 2, radius // 2, radius // 2, radius // 2))  # Left, right, up, down\n",
        "    distance_all = []\n",
        "    for i in range(0, radius, stride):  # Searching/matching in row-major order\n",
        "        for j in range(0, radius, stride):\n",
        "            distance_pix = torch.sum((y_pad[:, :, i:i + height, j:j + width] - x) ** 2, dim=1, keepdim=True)\n",
        "            distance_all += [F.avg_pool2d(distance_pix, patch_size, stride=1, padding=patch_size // 2)]\n",
        "\n",
        "    distance_all = torch.cat(distance_all, dim=1)      # Thus this stack of distances will be in row major order\n",
        "    location_min = torch.argmin(distance_all, dim=1)   # get the pixel/patch with the minimal distance\n",
        "    location_min = location_min * mask                  # Only need to match within the mask\n",
        "    distance_min_x = torch.fmod(location_min, radius) - radius // 2  # Need to adjust to take into account searching behind\n",
        "    distance_min_y = location_min / radius - radius // 2\n",
        "\n",
        "    grid_x = torch.arange(width).cuda().unsqueeze(0).unsqueeze(0) + distance_min_x.type(torch.float32)\n",
        "    grid_y = torch.arange(height).cuda().unsqueeze(1).unsqueeze(0) + distance_min_y.type(torch.float32)\n",
        "    grid_x = torch.clamp(grid_x.float() / width, 0, 1) * 2 - 1\n",
        "    grid_y = torch.clamp(grid_y.float() / height, 0, 1) * 2 - 1\n",
        "\n",
        "    grid = torch.stack([grid_x, grid_y], dim=3)\n",
        "    out = F.grid_sample(y, grid)\n",
        "    return out\n",
        "\n",
        "\n",
        "def match_color_histogram(x, y):\n",
        "    z = np.zeros_like(x)\n",
        "    shape = x[0].shape\n",
        "    for i in six.moves.range(len(x)):\n",
        "        a = x[i].reshape((3, -1))\n",
        "        a_mean = np.mean(a, axis=1, keepdims=True)\n",
        "        a_var = np.cov(a)\n",
        "        d, v = np.linalg.eig(a_var)\n",
        "        d += 1e-6\n",
        "        a_sigma_inv = v.dot(np.diag(d ** (-0.5))).dot(v.T)\n",
        "\n",
        "        b = y[i].reshape((3, -1))\n",
        "        b_mean = np.mean(b, axis=1, keepdims=True)\n",
        "        b_var = np.cov(b)\n",
        "        d, v = np.linalg.eig(b_var)\n",
        "        b_sigma = v.dot(np.diag(d ** 0.5)).dot(v.T)\n",
        "\n",
        "        transform = b_sigma.dot(a_sigma_inv)\n",
        "        z[i,:] = (transform.dot(a - a_mean) + b_mean).reshape(shape)\n",
        "    return z"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bsQF9k0VB-wk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}